From 9573f53de45cfedab3faaa6e1a128b3b90e3fc1a Mon Sep 17 00:00:00 2001
From: Merlin Kooshmanian <mkooshmanian@gmail.com>
Date: Mon, 8 Dec 2025 15:26:22 +0100
Subject: [PATCH 12/19] sched/tg_server: start/stop/update TG dl server and
 keep nr_running alive
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Start the task group’s dl_server when the first runnable task hits a CPU and
stop it when the last runnable task departs. Account runtime from update_se()
to keep the server in sync. Count TG servers in nr_running so NO_HZ doesn’t
stop the tick while a server runs on the physical rq.
---
 kernel/sched/core.c     | 118 +++++++++++++++++++++++++++++++++++++++-
 kernel/sched/deadline.c |  24 ++++++--
 kernel/sched/fair.c     |   4 ++
 kernel/sched/sched.h    |   3 +
 4 files changed, 143 insertions(+), 6 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 8c9e9b459036..26f7f23d2690 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2160,8 +2160,92 @@ unsigned long get_wchan(struct task_struct *p)
 	return ip;
 }
 
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+void tg_server_enqueue(struct rq *vrq, struct task_struct *p, int flags)
+{
+	struct task_group *tg = task_group(p);
+	int cpu;
+	struct sched_dl_entity *server;
+
+	if (!tg || tg == &root_task_group || !tg->tg_server)
+		return;
+
+	/*
+	 * ENQUEUE_DELAYED means the task never left the runqueue;
+	 * ignore the requeue so we do not double count.
+	 */
+	if (flags & ENQUEUE_DELAYED)
+		return;
+
+	cpu = cpu_of(vrq);
+	server = tg->tg_server[cpu];
+	if (!server)
+		return;
+
+	BUG_ON(vrq != server->vrq);
+
+	if (READ_ONCE(vrq->nr_running) == 1)
+		dl_server_start(server);
+}
+
+void tg_server_dequeue(struct rq *vrq, struct task_struct *p)
+{
+	struct task_group *tg = task_group(p);
+	int cpu;
+	struct sched_dl_entity *server;
+
+	if (!tg || tg == &root_task_group || !tg->tg_server)
+		return;
+
+	cpu = cpu_of(vrq);
+	server = tg->tg_server[cpu];
+	if (!server)
+		return;
+
+	BUG_ON(vrq != server->vrq);
+
+	if (!READ_ONCE(vrq->nr_running))
+		dl_server_stop(server);
+}
+
+void tg_server_account_runtime(struct rq *rq,
+					     struct task_struct *p,
+					     s64 delta_exec)
+{
+	struct task_group *tg;
+	struct sched_dl_entity *server;
+	int cpu;
+
+	if (unlikely(delta_exec <= 0))
+		return;
+
+	tg = task_group(p);
+	if (!tg || tg == &root_task_group || !tg->tg_server)
+		return;
+
+	cpu = cpu_of(rq);
+	server = READ_ONCE(tg->tg_server[cpu]);
+	if (!server)
+		return;
+
+	dl_server_update(server, delta_exec);
+}
+#endif
+
 void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 {
+	struct rq *task_rq = rq;
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+	struct rq_flags vrf;
+	bool class_locked = false;
+
+	task_rq = tg_server_rq_of_task(rq, p);
+	if (task_rq != rq) {
+		tg_server_vrq_lock(task_rq, &vrf);
+		class_locked = true;
+	}
+#endif
+
 	if (!(flags & ENQUEUE_NOCLOCK))
 		update_rq_clock(rq);
 
@@ -2172,7 +2256,7 @@ void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 	 */
 	uclamp_rq_inc(rq, p, flags);
 
-	p->sched_class->enqueue_task(rq, p, flags);
+	p->sched_class->enqueue_task(task_rq, p, flags);
 
 	psi_enqueue(p, flags);
 
@@ -2181,6 +2265,13 @@ void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 
 	if (sched_core_enabled(rq))
 		sched_core_enqueue(rq, p);
+
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+	tg_server_enqueue(task_rq, p, flags);
+	if (class_locked) {
+		tg_server_vrq_unlock(task_rq, &vrf);
+	}
+#endif
 }
 
 /*
@@ -2188,6 +2279,18 @@ void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
  */
 inline bool dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 {
+	bool ret;
+	struct rq *task_rq = rq;
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+	struct rq_flags vrf;
+	bool class_locked = false;
+	task_rq = tg_server_rq_of_task(rq, p);
+	if (task_rq != rq) {
+		tg_server_vrq_lock(task_rq, &vrf);
+		class_locked = true;
+	}
+#endif
+
 	if (sched_core_enabled(rq))
 		sched_core_dequeue(rq, p, flags);
 
@@ -2204,7 +2307,18 @@ inline bool dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 	 * and mark the task ->sched_delayed.
 	 */
 	uclamp_rq_dec(rq, p);
-	return p->sched_class->dequeue_task(rq, p, flags);
+
+	ret = p->sched_class->dequeue_task(task_rq, p, flags);
+
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+	if (ret)
+		tg_server_dequeue(task_rq, p);
+	if (class_locked) {
+		tg_server_vrq_unlock(task_rq, &vrf);
+	}
+#endif
+
+	return ret;
 }
 
 void activate_task(struct rq *rq, struct task_struct *p, int flags)
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 36c32b7a3c56..7a0b75218342 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -1946,11 +1946,22 @@ static inline
 void inc_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
 {
 	u64 deadline = dl_se->deadline;
+	struct rq *rq = rq_of_dl_rq(dl_rq);
 
 	dl_rq->dl_nr_running++;
 
-	if (!dl_server(dl_se))
-		add_nr_running(rq_of_dl_rq(dl_rq), 1);
+	if (!dl_server(dl_se)) {
+		add_nr_running(rq, 1);
+	} else if (dl_se != &rq->fair_server) {
+		/*
+		 * TG bandwidth servers live on the physical rq's DL queue.
+		 * Count them so nr_running stays non-zero while they execute,
+		 * otherwise NO_HZ idle may incorrectly stop the tick.
+		 * Do not count the rq-local fair_server to avoid double
+		 * accounting CFS load.
+		 */
+		add_nr_running(rq, 1);
+	}
 
 	inc_dl_deadline(dl_rq, deadline);
 }
@@ -1958,11 +1969,16 @@ void inc_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
 static inline
 void dec_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
 {
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+
 	WARN_ON(!dl_rq->dl_nr_running);
 	dl_rq->dl_nr_running--;
 
-	if (!dl_server(dl_se))
-		sub_nr_running(rq_of_dl_rq(dl_rq), 1);
+	if (!dl_server(dl_se)) {
+		sub_nr_running(rq, 1);
+	} else if (dl_se != &rq->fair_server) {
+		sub_nr_running(rq, 1);
+	}
 
 	dec_dl_deadline(dl_rq, dl_se->deadline);
 }
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 2467136d9c4e..bc6973cece66 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1193,6 +1193,10 @@ static s64 update_se(struct rq *rq, struct sched_entity *se)
 
 		/* cgroup time is always accounted against the donor */
 		cgroup_account_cputime(donor, delta_exec);
+
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+		tg_server_account_runtime(rq, donor, delta_exec);
+#endif
 	} else {
 		/* If not task, account the time against donor se  */
 		se->sum_exec_runtime += delta_exec;
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 64ac0d489385..894c48df1318 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -425,6 +425,9 @@ extern struct rq *vrq_of_tg(struct task_group *tg, int cpu);
 extern void tg_server_vrq_lock(struct rq *rq, struct rq_flags *rf);
 extern void tg_server_vrq_unlock(struct rq *rq, struct rq_flags *rf);
 u64 sync_vrq_clock(struct rq *vrq, struct rq *phys);
+extern void tg_server_enqueue(struct rq *vrq, struct task_struct *p, int flags);
+extern void tg_server_dequeue(struct rq *vrq, struct task_struct *p);
+extern void tg_server_account_runtime(struct rq *rq, struct task_struct *p, s64 delta_exec);
 #endif
 
 extern void dl_server_update_idle_time(struct rq *rq,
-- 
2.43.0

