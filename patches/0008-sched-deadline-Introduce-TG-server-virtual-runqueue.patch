From 95cff7c5dd139b5b81311e60ce22acd899cb0ba6 Mon Sep 17 00:00:00 2001
From: Merlin Kooshmanian <mkooshmanian@gmail.com>
Date: Mon, 8 Dec 2025 11:19:35 +0100
Subject: [PATCH 08/19] sched/deadline: Introduce TG server virtual runqueue

Introduce a per-server virtual rq for CONFIG_TG_BANDWIDTH_SERVER and keep it
within the sched_dl_entity. The virtual rq mirrors the CPU rq clock state so
that task group work can be queued there instead of the CPU rq.

Allocate and free the virtual rq together with the bandwidth server, and add
a helper to retrieve a TG's virtual rq from its task group and CPU.

init_tg_dl_se() now starts the server when the virtual rq already has tasks,
preparing for upcoming enqueue changes. Simplify tg_has_tasks() since runtime
updates no longer need to distinguish active tasks once the virtual rq hosts
the runnable set.
---
 include/linux/sched.h   |   1 +
 kernel/sched/core.c     | 160 ++++++++++++++++++++++++++++++++++------
 kernel/sched/deadline.c |  11 ++-
 kernel/sched/sched.h    |  23 +++++-
 4 files changed, 167 insertions(+), 28 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff00d2356cc2..198dccc7e8bf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -735,6 +735,7 @@ struct sched_dl_entity {
 	dl_server_pick_f		server_pick_task;
 
 #ifdef CONFIG_TG_BANDWIDTH_SERVER
+	struct rq			*vrq;
 	struct sched_dl_entity		*parent;
 	struct task_group		*tg;
 #endif
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 3eb3874d0f36..17f5f6e91fbd 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2051,6 +2051,22 @@ static inline void uclamp_post_fork(struct task_struct *p) { }
 static inline void init_uclamp(void) { }
 #endif /* !CONFIG_UCLAMP_TASK */
 
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+struct rq *vrq_of_tg(struct task_group *tg, int cpu)
+{
+	struct sched_dl_entity *server;
+
+	if (!tg || tg == &root_task_group || !tg->tg_server)
+		return NULL;
+
+	server = READ_ONCE(tg->tg_server[cpu]);
+	if (!server)
+		return NULL;
+
+	return READ_ONCE(server->vrq);
+}
+#endif
+
 bool sched_task_on_rq(struct task_struct *p)
 {
 	return task_on_rq_queued(p);
@@ -9179,7 +9195,7 @@ static int tg_check_root_dl_bandwidth(unsigned long total_bw)
 	return 1;
 }
 
-bool tg_has_tasks(struct task_group *tg, bool active_only)
+static bool tg_has_tasks(struct task_group *tg)
 {
 	struct css_task_iter it;
 	struct task_struct *task;
@@ -9187,15 +9203,8 @@ bool tg_has_tasks(struct task_group *tg, bool active_only)
 
 	css_task_iter_start(&tg->css, 0, &it);
 	while ((task = css_task_iter_next(&it))) {
-		if (!active_only) {
-			ret = true;
-			break;
-		}
-
-		if (task_on_rq_queued(task) || task_is_running(task)) {
-			ret = true;
-			break;
-		}
+		ret = true;
+		break;
 	}
 	css_task_iter_end(&it);
 
@@ -9232,11 +9241,12 @@ tg_bandwidth_server_pick_task(struct sched_dl_entity *dl_se)
 	return NULL;
 }
 
-void init_tg_bandwidth_entry(struct task_group *tg, struct rq *rq,
+void init_tg_bandwidth_entry(struct task_group *tg, struct rq *vrq,
 		struct sched_dl_entity *server, int cpu,
 		struct sched_dl_entity *parent)
 {
 	server->tg = tg;
+	server->vrq = vrq;
 	server->parent = parent;
 	tg->tg_server[cpu] = server;
 }
@@ -9248,6 +9258,89 @@ void init_tg_bandwidth(struct dl_bandwidth *dl_bw, u64 period, u64 runtime)
 	dl_bw->dl_runtime = runtime;
 }
 
+static struct rq *alloc_virtual_rq(struct task_group *tg, int cpu,
+				   struct rq *template)
+{
+	struct rq *vrq;
+	u64 now = sched_clock_cpu(cpu);
+
+	vrq = kzalloc_node(sizeof(*vrq), GFP_KERNEL, cpu_to_node(cpu));
+	if (!vrq)
+		return NULL;
+
+	raw_spin_lock_init(&vrq->__lock);
+	vrq->nr_running = 0;
+	vrq->calc_load_active = 0;
+	vrq->calc_load_update = jiffies + LOAD_FREQ;
+
+	/*
+	 * Virtual runqueues do not drive the hardware clock, but load-accounting
+	 * helpers expect rq_clock() assertions to pass. Pretend we're indefinitely
+	 * skipping clock updates.
+	 */
+	vrq->clock_update_flags = RQCF_ACT_SKIP;
+	vrq->clock = now;
+	vrq->clock_task = now;
+	vrq->clock_pelt = 0;
+	vrq->clock_pelt_idle = 0;
+	vrq->clock_idle = 0;
+
+	init_cfs_rq(&vrq->cfs);
+	init_rt_rq(&vrq->rt);
+	init_dl_rq(&vrq->dl);
+
+	INIT_LIST_HEAD(&vrq->cfs_tasks);
+
+	vrq->cpu = cpu;
+	vrq->online = 0;
+	vrq->idle = idle_task(cpu);
+	vrq->curr = vrq->idle;
+	vrq->rd = template ? template->rd : NULL;
+	vrq->sd = template ? template->sd : NULL;
+	vrq->cpu_capacity = template ? template->cpu_capacity : SCHED_CAPACITY_SCALE;
+	vrq->balance_callback = NULL;
+	atomic_set(&vrq->nr_iowait, 0);
+#ifdef CONFIG_NO_HZ_COMMON
+	atomic_set(&vrq->nohz_flags, 0);
+#endif
+#ifdef CONFIG_SCHED_CORE
+	vrq->core = vrq;
+	vrq->core_pick = NULL;
+	vrq->core_dl_server = NULL;
+	vrq->core_enabled = 0;
+	vrq->core_tree = RB_ROOT;
+	vrq->core_forceidle_count = 0;
+	vrq->core_forceidle_occupation = 0;
+	vrq->core_forceidle_start = 0;
+	vrq->core_cookie = 0UL;
+#endif
+
+	hrtick_rq_init(vrq);
+
+	if (!zalloc_cpumask_var_node(&vrq->scratch_mask, GFP_KERNEL,
+				     cpu_to_node(cpu)))
+		goto free_rq;
+
+	return vrq;
+
+free_rq:
+	kfree(vrq);
+	return NULL;
+}
+
+static void free_virtual_rq(struct rq *vrq)
+{
+	if (!vrq)
+		return;
+
+	if (cpumask_available(vrq->scratch_mask))
+		free_cpumask_var(vrq->scratch_mask);
+#ifdef CONFIG_SCHED_HRTICK
+	hrtimer_cancel(&vrq->hrtick_timer);
+#endif
+	kfree(vrq);
+}
+
 int alloc_tg_bandwidth_server(struct task_group *tg, struct task_group *parent)
 {
 	int cpu;
@@ -9261,7 +9354,8 @@ int alloc_tg_bandwidth_server(struct task_group *tg, struct task_group *parent)
 
 	for_each_possible_cpu(cpu) {
 		struct sched_dl_entity *server;
-		struct rq *rq = cpu_rq(cpu);
+		struct rq *parent_rq = cpu_rq(cpu);
+		struct rq *vrq;
 		struct sched_dl_entity *parent_server =
 			parent ? parent->tg_server[cpu] : NULL;
 
@@ -9269,6 +9363,12 @@ int alloc_tg_bandwidth_server(struct task_group *tg, struct task_group *parent)
 		if (!server)
 			goto err_free;
 
+		vrq = alloc_virtual_rq(tg, cpu, parent_rq);
+		if (!vrq) {
+			kfree(server);
+			goto err_free;
+		}
+
 		init_dl_entity(server);
 		server->dl_runtime = tg->tg_bandwidth.dl_runtime;
 		server->dl_period = tg->tg_bandwidth.dl_period;
@@ -9277,16 +9377,24 @@ int alloc_tg_bandwidth_server(struct task_group *tg, struct task_group *parent)
 		server->dl_density = to_ratio(server->dl_period, server->dl_runtime);
 		server->dl_server = 1;
 
-		dl_server_init(server, rq, tg_bandwidth_server_pick_task);
+		dl_server_init(server, parent_rq, tg_bandwidth_server_pick_task);
 
-		init_tg_bandwidth_entry(tg, rq, server, cpu, parent_server);
+		init_tg_bandwidth_entry(tg, vrq, server, cpu, parent_server);
 	}
 
 	return 1;
 
 err_free:
 	for_each_possible_cpu(cpu) {
-		kfree(tg->tg_server[cpu]);
+		struct sched_dl_entity *server = tg->tg_server ? tg->tg_server[cpu] : NULL;
+		struct rq *vrq;
+
+		if (!server)
+			continue;
+
+		vrq = server->vrq;
+		free_virtual_rq(vrq);
+		kfree(server);
 		tg->tg_server[cpu] = NULL;
 	}
 	kfree(tg->tg_server);
@@ -9304,7 +9412,10 @@ void free_tg_bandwidth_server(struct task_group *tg)
 		return;
 
 	for_each_possible_cpu(cpu) {
-		if (!tg->tg_server[cpu])
+		struct sched_dl_entity *server = tg->tg_server[cpu];
+		struct rq *vrq;
+
+		if (!server)
 			continue;
 
 		/*
@@ -9315,13 +9426,16 @@ void free_tg_bandwidth_server(struct task_group *tg)
 		 * Fix this issue by changing the group runtime
 		 * to 0 immediately before freeing it.
 		 */
-		if (tg->tg_server[cpu]->dl_runtime)
-			init_tg_dl_se(tg, cpu, 0, tg->tg_server[cpu]->dl_period);
+		if (server->dl_runtime)
+			init_tg_dl_se(tg, cpu, 0, server->dl_period);
+
+		vrq = server->vrq;
+		raw_spin_rq_lock_irqsave(vrq, flags);
+		hrtimer_cancel(&server->dl_timer);
+		raw_spin_rq_unlock_irqrestore(vrq, flags);
 
-		raw_spin_rq_lock_irqsave(cpu_rq(cpu), flags);
-		hrtimer_cancel(&tg->tg_server[cpu]->dl_timer);
-		raw_spin_rq_unlock_irqrestore(cpu_rq(cpu), flags);
-		kfree(tg->tg_server[cpu]);
+		free_virtual_rq(vrq);
+		kfree(server);
 		tg->tg_server[cpu] = NULL;
 	}
 
@@ -9383,7 +9497,7 @@ static int tg_check_dl_bandwidth_constraints(struct task_group *tg, void *data)
 		return -EINVAL;
 
 	if (dl_bandwidth_enabled() && !runtime && cur_runtime &&
-	    tg_has_tasks(tg, false))
+	    tg_has_tasks(tg))
 		return -EBUSY;
 
 	total = to_ratio(period, runtime);
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 3feb75e6f041..11f07da0a2c7 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -397,9 +397,11 @@ void init_tg_dl_se(struct task_group *tg, int cpu, u64 runtime, u64 period)
 {
 	struct sched_dl_entity *dl_se;
 	struct rq *rq;
+	struct rq *vrq;
 	struct dl_rq *dl_rq;
 	struct sched_dl_entity *parent;
-	bool has_active_tasks, is_active_group;
+	bool is_active_group;
+	unsigned int nr_task_running;
 	u64 old_runtime;
 	unsigned long new_bw;
 
@@ -411,9 +413,12 @@ void init_tg_dl_se(struct task_group *tg, int cpu, u64 runtime, u64 period)
 		return;
 
 	rq = dl_se->rq;
+	vrq = dl_se->vrq;
 	dl_rq = dl_rq_of_se(dl_se);
+	BUG_ON(!vrq);
+
 	is_active_group = is_active_sched_group(tg);
-	has_active_tasks = tg_has_tasks(tg, true);
+	nr_task_running = READ_ONCE(vrq->nr_running);
 
 	raw_spin_rq_lock_irq(rq);
 	update_rq_clock(rq);
@@ -444,7 +449,7 @@ void init_tg_dl_se(struct task_group *tg, int cpu, u64 runtime, u64 period)
 			__sub_rq_bw(parent->dl_bw, dl_rq);
 	}
 
-	if (has_active_tasks)
+	if (nr_task_running)
 		dl_server_start(dl_se);
 
 	raw_spin_rq_unlock_irq(rq);
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3dc8dcc42fa5..ea3ec23b40b0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -81,6 +81,7 @@ struct cfs_rq;
 struct rt_rq;
 struct sched_group;
 struct cpuidle_state;
+struct task_group;
 
 #ifdef CONFIG_PARAVIRT
 # include <asm/paravirt.h>
@@ -418,6 +419,10 @@ extern void dl_server_init(struct sched_dl_entity *dl_se, struct rq *rq,
 		    dl_server_pick_f pick_task);
 extern void sched_init_dl_servers(void);
 
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+extern struct rq *vrq_of_tg(struct task_group *tg, int cpu);
+#endif
+
 extern void dl_server_update_idle_time(struct rq *rq,
 		    struct task_struct *p);
 extern void fair_server_init(struct rq *rq);
@@ -573,12 +578,11 @@ static inline struct task_group *css_tg(struct cgroup_subsys_state *css)
 extern int tg_nop(struct task_group *tg, void *data);
 
 #ifdef CONFIG_TG_BANDWIDTH_SERVER
-extern void init_tg_bandwidth_entry(struct task_group *tg, struct rq *rq,
+extern void init_tg_bandwidth_entry(struct task_group *tg, struct rq *vrq,
 		struct sched_dl_entity *server, int cpu,
 		struct sched_dl_entity *parent);
 extern void init_tg_bandwidth(struct dl_bandwidth *dl_bw, u64 period, u64 runtime);
 extern void init_tg_dl_se(struct task_group *tg, int cpu, u64 runtime, u64 period);
-extern bool tg_has_tasks(struct task_group *tg, bool active_only);
 extern unsigned long tg_root_bandwidth_sum(void);
 extern int sched_group_set_tg_runtime(struct task_group *tg, long runtime_us);
 extern int sched_group_set_tg_period(struct task_group *tg, u64 period_us);
@@ -2225,6 +2229,21 @@ static inline struct task_group *task_group(struct task_struct *p)
 
 #endif /* !CONFIG_CGROUP_SCHED */
 
+#ifdef CONFIG_TG_BANDWIDTH_SERVER
+static inline struct rq *tg_server_rq_of_task(struct rq *rq,
+					      struct task_struct *p)
+{
+	struct task_group *tg = task_group(p);
+	struct rq *vrq;
+
+	if (!tg || tg == &root_task_group || !tg->tg_server)
+		return rq;
+
+	vrq = vrq_of_tg(tg, cpu_of(rq));
+	return vrq ? vrq : rq;
+}
+#endif
+
 static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 {
 	set_task_rq(p, cpu);
-- 
2.43.0

